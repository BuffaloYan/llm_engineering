{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# Gradio Day!\n",
    "\n",
    "Today we will build User Interfaces using the outrageously simple Gradio framework.\n",
    "\n",
    "Prepare for joy!\n",
    "\n",
    "Please note: your Gradio screens may appear in 'dark mode' or 'light mode' depending on your computer settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()\n",
    "\n",
    "class LLM_Model:\n",
    "    api_key: str\n",
    "    model: str\n",
    "    base_url: str\n",
    "    openai: OpenAI\n",
    "\n",
    "    def __init__(self, key, model, url):\n",
    "        self.api_key_name = key\n",
    "        self.model = model\n",
    "        self.base_url = url\n",
    "        self.api_key = os.getenv(key)\n",
    "        if not self.api_key:\n",
    "            raise ValueError (\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "        self.openai = OpenAI(base_url=url, api_key=self.api_key)\n",
    "\n",
    "    def getResponse(self, messages, streaming=False, tools=None):\n",
    "        response = self.openai.chat.completions.create(model=self.model, messages=messages, stream=streaming, tools=tools)\n",
    "        if streaming == False:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            result = \"\"\n",
    "            for chunk in response:\n",
    "                result += chunk.choices[0].delta.content or \"\"\n",
    "                yield result\n",
    "        \n",
    "deepseek = LLM_Model(\"DEEPSEEK_API_KEY\", \"deepseek-chat\", \"https://api.deepseek.com\")\n",
    "deepseekR1 = LLM_Model(\"DEEPSEEK_API_KEY\", \"deepseek-reasoner\", \"https://api.deepseek.com\")\n",
    "llama = LLM_Model(\"OPENAI_API_KEY\", \"llama3.2\", \"http://localhost:11434/v1\")\n",
    "gpt = LLM_Model(\"OPENAI_API_KEY\", \"gpt-4o\", \"https://api.openai.com/v1\")\n",
    "\n",
    "llms = {\"DeepSeek\": deepseek, \"DeepSeek R1\": deepseekR1, \"Llama\": llama, \"ChatGpt\": gpt}\n",
    "\n",
    "# A generic system message - no more snarky adversarial AIs!\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a call that streams back results\n",
    "# If you'd like a refresher on Generators (the \"yield\" keyword),\n",
    "# Please take a look at the Intermediate Python notebook in week1 folder.\n",
    "\n",
    "def stream_openai(prompt, model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    return model.getResponse(messages, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc8e930-ba2a-4194-8f7c-044659150626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_openai(prompt, gpt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    elif model==\"DeepSeekV3\":\n",
    "        result = stream_openai(prompt, deepseek)\n",
    "    elif model==\"DeepSeekR1\":\n",
    "        result = stream_openai(prompt, deepseekR1)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6), gr.Dropdown([\"GPT\", \"Claude\", \"DeepSeekV3\", \"DeepSeekR1\"], label=\"Select model\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
