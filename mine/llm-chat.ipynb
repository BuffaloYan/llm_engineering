{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# Gradio Day!\n",
    "\n",
    "Today we will build User Interfaces using the outrageously simple Gradio framework.\n",
    "\n",
    "Prepare for joy!\n",
    "\n",
    "Please note: your Gradio screens may appear in 'dark mode' or 'light mode' depending on your computer settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()\n",
    "\n",
    "class LLM_Model:\n",
    "    api_key: str\n",
    "    model: str\n",
    "    base_url: str\n",
    "    openai: OpenAI\n",
    "\n",
    "    def __init__(self, key, model, url):\n",
    "        self.api_key_name = key\n",
    "        self.model = model\n",
    "        self.base_url = url\n",
    "        self.api_key = os.getenv(key)\n",
    "        if not self.api_key:\n",
    "            raise ValueError (\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "        self.openai = OpenAI(base_url=url, api_key=self.api_key)\n",
    "\n",
    "    def getResponse(self, messages, streaming=False, tools=None):\n",
    "        response = self.openai.chat.completions.create(model=self.model, messages=messages, stream=streaming, tools=tools)\n",
    "        if streaming == False:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            result = \"\"\n",
    "            for chunk in response:\n",
    "                result += chunk.choices[0].delta.content or \"\"\n",
    "                yield result\n",
    "        \n",
    "deepseek = LLM_Model(\"DEEPSEEK_API_KEY\", \"deepseek-chat\", \"https://api.deepseek.com\")\n",
    "deepseekR1 = LLM_Model(\"DEEPSEEK_API_KEY\", \"deepseek-reasoner\", \"https://api.deepseek.com\")\n",
    "llama = LLM_Model(\"OPENAI_API_KEY\", \"llama3.2\", \"http://localhost:11434/v1\")\n",
    "gpt_4o = LLM_Model(\"OPENAI_API_KEY\", \"gpt-4o-mini\", \"https://api.openai.com/v1\")\n",
    "gpt_o3 = LLM_Model(\"OPENAI_API_KEY\", \"o3-mini\", \"https://api.openai.com/v1\")\n",
    "qwen = LLM_Model(\"QWEN_API_KEY\", \"qwen-max-2025-01-25\", \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\")\n",
    "\n",
    "llms = {\"DeepSeek\": deepseek, \"DeepSeek R1\": deepseekR1, \"Llama\": llama, \"ChatGpt\": gpt_4o, \"Qwen\": qwen}\n",
    "\n",
    "\n",
    "# A generic system message - no more snarky adversarial AIs!\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a call that streams back results\n",
    "# If you'd like a refresher on Generators (the \"yield\" keyword),\n",
    "# Please take a look at the Intermediate Python notebook in week1 folder.\n",
    "\n",
    "def stream_openai(prompt, model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    return model.getResponse(messages, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc8e930-ba2a-4194-8f7c-044659150626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    \"GPT-4o-mini\": gpt_4o,\n",
    "    \"GPT-o3-mini\": gpt_o3,\n",
    "    \"QWen-2.5-max\": qwen,\n",
    "    \"DeepSeekV3\": deepseek,\n",
    "    \"DeepSeekR1\": deepseekR1\n",
    "}\n",
    "\n",
    "def stream_model(prompt, model):\n",
    "    if dict_models[model]:\n",
    "        result = stream_openai(prompt, dict_models[model])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Your message:\", lines=6), \n",
    "        gr.Dropdown(dict_models.keys(), \n",
    "                    label=\"Select model\", \n",
    "                    value=\"GPT-4o-mini\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47930dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# create a new Chat with OpenAI\n",
    "dict_models2 = {\n",
    "    \"GPT-4o-mini\": ConversationChain(memory = memory,\n",
    "                                     llm=ChatOpenAI(model=gpt_4o.model, disable_streaming=False, streaming=True, api_key=gpt_4o.api_key, base_url=gpt_4o.base_url)),\n",
    "    \"GPT-o3-mini\": ConversationChain(memory = memory,\n",
    "                                     llm=ChatOpenAI(model=gpt_o3.model, disable_streaming=False, api_key=gpt_o3.api_key, base_url=gpt_o3.base_url)),\n",
    "    \"QWen-2.5-max\": ConversationChain(memory = memory,\n",
    "                                      llm=ChatOpenAI(model=qwen.model, disable_streaming=False, api_key=qwen.api_key, base_url=qwen.base_url)),\n",
    "    \"DeepSeekV3\": ConversationChain(memory = memory,\n",
    "                                    llm=ChatOpenAI(model=deepseek.model, disable_streaming=False, api_key=deepseek.api_key, base_url=deepseek.base_url)),\n",
    "    \"DeepSeekR1\": ConversationChain(memory = memory,\n",
    "                                    llm=ChatOpenAI(model=deepseekR1.model, disable_streaming=False, api_key=deepseekR1.api_key, base_url=deepseekR1.base_url))\n",
    "}\n",
    "\n",
    "conversation = dict_models2[\"GPT-4o-mini\"]\n",
    "\n",
    "def chat(message):\n",
    "    result = conversation.predict(input=message)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Define a function to handle streaming responses\n",
    "def chat_with_ai(user_input, history, model):\n",
    "    conversation = dict_models2[model]\n",
    "    history = history or []  # Initialize history if None\n",
    "    response = \"\"\n",
    "    for chunk in conversation.stream(input=user_input):\n",
    "        response += chunk[\"response\"]\n",
    "        yield history + [(user_input, response)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017edbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# LangChain Chatbot with Streaming\")\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "    user_input = gr.Textbox(label=\"Your Message\")\n",
    "    models = gr.Dropdown(dict_models.keys(), \n",
    "                    label=\"Select model\", \n",
    "                    value=\"GPT-4o-mini\")\n",
    "    clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "    # Chat function\n",
    "    def respond(user_input, history, model):\n",
    "        # Call the chat_with_ai function and yield updates\n",
    "        for updated_history in chat_with_ai(user_input, history, model):\n",
    "            yield updated_history, \"\"  # Clear the input field after submission\n",
    "\n",
    "    user_input.submit(\n",
    "        respond,  # Function to call\n",
    "        [user_input, chatbot, models],  # Inputs\n",
    "        [chatbot, user_input],  # Outputs (update both chatbot and streaming response)\n",
    "        show_progress=True,  # Show progress while streaming\n",
    "    )\n",
    "    clear_button.click(lambda: None, None, chatbot, queue=False)  # Clear chat history\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
