{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "openai.base_url = \"https://api.deepseek.com\"  # Set custom base URL\n",
    "MODEL_NAME = \"deepseek-chat\"  # Confirm model name with DeepSeek docs\n",
    "\n",
    "def format_history(chat_history: List[Tuple[str, str]]) -> List[dict]:\n",
    "    \"\"\"Convert Gradio history to API-compatible format\"\"\"\n",
    "    messages = []\n",
    "    for user_msg, bot_msg in chat_history:\n",
    "        if bot_msg is not None:\n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": bot_msg}\n",
    "            ])\n",
    "    return messages\n",
    "\n",
    "def stream_response(messages: List[dict]) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream response using DeepSeek API\"\"\"\n",
    "    try:\n",
    "        full_response = \"\"\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                full_response += content\n",
    "                yield full_response\n",
    "\n",
    "    except Exception as e:\n",
    "        yield f\"API Error: {str(e)}\"\n",
    "\n",
    "def chat_response(message: str, history: List[Tuple[str, str]]) -> Generator[List[Tuple[str, str]], None, None]:\n",
    "    \"\"\"Handle chat interaction with streaming\"\"\"\n",
    "    base_messages = format_history(history)\n",
    "    base_messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream_response(base_messages):\n",
    "        full_response = chunk\n",
    "        yield history + [(message, full_response)]\n",
    "\n",
    "    formatted_response = format_markdown(full_response)\n",
    "    yield history + [(message, formatted_response)]\n",
    "\n",
    "def format_markdown(text: str) -> str:\n",
    "    \"\"\"Format response with markdown support\"\"\"\n",
    "    return text.replace(\"\\n\", \"  \\n\")  # Preserve line breaks\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {max-width: 800px}\") as app:\n",
    "    gr.Markdown(\"# DeepSeek Chat Clone\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        elem_id=\"chatbot\",\n",
    "        height=700,\n",
    "        show_copy_button=True,\n",
    "        avatar_images=(None, \"/path/to/deepseek-logo.png\")\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            label=\"Message\",\n",
    "            placeholder=\"Ask me anything...\",\n",
    "            container=False,\n",
    "            autofocus=True,\n",
    "            scale=7\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
    "\n",
    "    def user(message: str, history: list) -> Tuple[str, list]:\n",
    "        return \"\", history + [(message, None)]\n",
    "\n",
    "    # Event handlers\n",
    "    msg.submit(\n",
    "        user, [msg, chatbot], [msg, chatbot], queue=False\n",
    "    ).then(\n",
    "        chat_response, [msg, chatbot], [chatbot]\n",
    "    )\n",
    "\n",
    "    submit_btn.click(\n",
    "        user, [msg, chatbot], [msg, chatbot], queue=False\n",
    "    ).then(\n",
    "        chat_response, [msg, chatbot], [chatbot]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        lambda: [], None, chatbot, queue=False\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch(share=True)  # Always share with auto port selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
